<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Reading Eye For The Blind With NVIDIA Jetson Nano · Andrew Bernas</title><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="generator" content="Docusaurus"/><meta name="description" content="December 2019 - February 2020"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Reading Eye For The Blind With NVIDIA Jetson Nano · Andrew Bernas"/><meta property="og:type" content="website"/><meta property="og:url" content="https://bandofpv.github.io/"/><meta property="og:description" content="December 2019 - February 2020"/><meta property="og:image" content="https://bandofpv.github.io/img/favicon.ico"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://bandofpv.github.io/img/favicon.ico"/><link rel="shortcut icon" href="/img/favicon.ico"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/favicon.ico" alt="Andrew Bernas"/><h2 class="headerTitleWithLogo">Andrew Bernas</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/about-me" target="_self">About Me</a></li><li class=""><a href="/docs/in-progress/sar-drone" target="_self">Projects</a></li><li class="siteNavGroupActive"><a href="/docs/tutorials/sar-drone" target="_self">Tutorials</a></li><li class=""><a href="/docs/gallery/gallery-best" target="_self">Photography</a></li><li class=""><a href="https://docs.google.com/document/d/1LFxUAfSPOlgA_n8IyqA-czno1DAukBsEL19MBdMJNjs/edit?usp=sharing" target="_self">Resume</a></li><li class=""><a href="https://www.youtube.com/channel/UCYIknwUG33u7_Se2__GrHrg" target="_self">Youtube</a></li><li class=""><a href="https://github.com/bandofpv" target="_self">GitHub</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>Tutorials</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle collapsible">Tutorials<span class="arrow"><svg width="24" height="24" viewBox="0 0 24 24"><path fill="#565656" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path d="M0 0h24v24H0z" fill="none"></path></svg></span></h3><ul class="hide"><li class="navListItem"><a class="navItem" href="/docs/tutorials/sar-drone">Search And Rescue Drone</a></li><li class="navListItem navListItemActive"><a class="navItem" href="/docs/tutorials/reading-eye-for-the-blind">Reading Eye For The Blind</a></li><li class="navListItem"><a class="navItem" href="/docs/tutorials/recycle-sorting-robot">Recycle Sorting Robot</a></li><li class="navListItem"><a class="navItem" href="/docs/tutorials/ft-explorer-vtol">FT Explorer VTOL</a></li><li class="navListItem"><a class="navItem" href="/docs/tutorials/multirotor-beginner-series">Multirotor Beginner Series</a></li><li class="navListItem"><a class="navItem" href="/docs/tutorials/qav-ulx-tutorial">QAV-ULX Build</a></li><li class="navListItem"><a class="navItem" href="/docs/tutorials/qav-r-tutorial">QAV-R Build</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer docsContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1 id="__docusaurus" class="postHeaderTitle">Reading Eye For The Blind With NVIDIA Jetson Nano</h1></header><article><div><span><p>December 2019 - February 2020</p>
<p><a href="https://www.hackster.io/bandofpv/reading-eye-for-the-blind-with-nvidia-jetson-nano-8657ed">Hackster.io Tutorial</a></p>
<h2><a class="anchor" aria-hidden="true" id="story"></a><a href="#story" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Story:</h2>
<p>The World Health Organization (WHO) estimates that there are 285 million people that are visually impaired. Specifically in the United States, there are 1.3 million who are legally blind and only 10 percent of them can actually read braille and only 10 percent of the blind children are learning it. This is due to how expensive books are to read in braille. It costs up to $15,000 just to convert five chapters of a science book! Due to the price, very few blind people are able to learn through books. People who are reading impaired or suffer vision loss also struggle to read. While many can use audio-books, they are still limited on what they can read based on audio books' availability and costs. Books are the cheapest way to learn and 285 million people are unable to take advantage of this resource we greatly take for granted. The Reading Eye device would allow more freedom in terms of book choice, without having to make investments towards buying several audio-books. It is able to detect printed and handwritten text and speak it in a realistic synthesized voice.</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/story.jpg" alt="story Andrew Bernas"></p>
<p>My whole inspiration for this project was to help my grandmother who's vision degrades everyday due to age. I then thought of all the others who suffer due to bad vision or reading disabilities which motivated me to pursue this project.</p>
<h2><a class="anchor" aria-hidden="true" id="demo-video"></a><a href="#demo-video" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Demo Video:</h2>
<p><a href="https://www.youtube.com/watch?v=ZVquCjLMWuA"><img src="/docs/assets/tutorials/reading-eye-for-the-blind/demo-video.jpg" alt="demo-video Andrew Bernas"></a></p>
<h2><a class="anchor" aria-hidden="true" id="code"></a><a href="#code" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Code:</h2>
<ul>
<li><strong><a href="https://github.com/bandofpv/Reading_Eye_For_The_Blind">Reading_Eye_For_The_Blind GitHub Repository</a></strong></li>
<li><strong><a href="https://github.com/bandofpv/Handwritten_Text">Handwritten_Text GitHub Repository</a></strong></li>
</ul>
<p><strong>Note:</strong> When I refer to &quot;your computer&quot; I am referring to a Ubuntu 18.04 LTS machine. When I refer to &quot;your Jetson Nano&quot; I am referring to your Jetson Nano. While this tutorial can all be done using a Jetson Nano, I would not recommend it because it is slow during heavy processing compared to a traditional desktop machine.</p>
<h2><a class="anchor" aria-hidden="true" id="hardware-components"></a><a href="#hardware-components" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Hardware components:</h2>
<p><a href="https://www.hackster.io/products/buy/68111?s=BAhJIhMzMTU3ODksUHJvamVjdAY6BkVG%0A">Nvidia Jetson Nano Developer Kit</a> x1</p>
<p><a href="https://www.hackster.io/products/buy/69078?s=BAhJIhMzMTU3ODksUHJvamVjdAY6BkVG%0A">USB Keyboard and Mouse</a> x1</p>
<p><a href="https://www.hackster.io/products/buy/69079?s=BAhJIhMzMTU3ODksUHJvamVjdAY6BkVG%0A">5V 2.5A Power Supply With Micro USB Cable</a> x1</p>
<p><a href="https://www.hackster.io/products/buy/69080?s=BAhJIhMzMTU3ODksUHJvamVjdAY6BkVG%0A">32GB Micro SD Card</a> x1</p>
<p><a href="https://www.hackster.io/products/buy/69081?s=BAhJIhMzMTU3ODksUHJvamVjdAY6BkVG%0A">Computer Display</a> x1</p>
<p><a href="https://www.hackster.io/products/buy/68122?s=BAhJIhMzMTU3ODksUHJvamVjdAY6BkVG%0A">Raspberry Pi Camera Module V2</a> x1</p>
<p><a href="https://www.hackster.io/products/buy/68123?s=BAhJIhMzMTU3ODksUHJvamVjdAY6BkVG%0A">Arducam 8MP Wide Angle Drop-in Replacement</a> x1</p>
<p><a href="https://www.hackster.io/products/buy/68124?s=BAhJIhMzMTU3ODksUHJvamVjdAY6BkVG%0A">INIU Power Bank</a> x1</p>
<p><a href="https://www.hackster.io/products/buy/68125?s=BAhJIhMzMTU3ODksUHJvamVjdAY6BkVG%0A">Edimax Wi-Fi USB Adapter</a> x1</p>
<p><a href="https://www.hackster.io/products/buy/68126?s=BAhJIhMzMTU3ODksUHJvamVjdAY6BkVG%0A">USB to 3.5mm Jack Audio Adapter</a> x1</p>
<p><a href="https://www.hackster.io/products/buy/68127?s=BAhJIhMzMTU3ODksUHJvamVjdAY6BkVG%0A">2.0 A-Male to Micro B USB Cable</a> x1</p>
<p><a href="https://www.hackster.io/products/buy/68128?s=BAhJIhMzMTU3ODksUHJvamVjdAY6BkVG%0A">Mini LED Arcade Button</a> x1</p>
<p><a href="https://www.hackster.io/products/buy/68129?s=BAhJIhMzMTU3ODksUHJvamVjdAY6BkVG%0A">Illuminated Toggle Switch with Cover</a> x1</p>
<p><a href="https://www.hackster.io/products/buy/69201?s=BAhJIhMzMTU3ODksUHJvamVjdAY6BkVG%0A">Flex Cable for Raspberry Pi Camera</a> x1</p>
<p><a href="https://www.hackster.io/products/buy/68963?s=BAhJIhMzMTU3ODksUHJvamVjdAY6BkVG%0A">Noctua NF-A4x20 5V Fan</a> x1</p>
<p><a href="https://www.hackster.io/products/buy/68964?s=BAhJIhMzMTU3ODksUHJvamVjdAY6BkVG%0A">M3x25 Screws</a> x1</p>
<p><a href="https://www.hackster.io/products/buy/69203?s=BAhJIhMzMTU3ODksUHJvamVjdAY6BkVG%0A">Jumper Wires</a> x1</p>
<h2><a class="anchor" aria-hidden="true" id="hand-tools-and-fabrication-machines"></a><a href="#hand-tools-and-fabrication-machines" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Hand tools and fabrication machines:</h2>
<p>3D Printer</p>
<p>Soldering Iron</p>
<p>Hot Glue Gun</p>
<p>Screwdriver Set</p>
<h2><a class="anchor" aria-hidden="true" id="3d-files"></a><a href="#3d-files" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>3D Files:</h2>
<p><a href="https://sketchfab.com/3d-models/lid-f9c1e3893ed24451ad2f3feafa6f4ff7">Lid</a></p>
<p><a href="https://sketchfab.com/3d-models/box-a7cedc0e520542b5a34a0caec4282940">Box</a></p>
<p><a href="https://sketchfab.com/3d-models/camera-mount-7b7e8730115a4d55ab6061d22674058b">Camera Mount</a></p>
<h2><a class="anchor" aria-hidden="true" id="step-1-setting-up-your-computer"></a><a href="#step-1-setting-up-your-computer" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Step 1, Setting Up Your Computer:</h2>
<p>As stated above, I recommend a clean install of Ubuntu 18.04 LTS. Instructions on installing Ubuntu can be found <a href="https://www.youtube.com/watch?v=u5QyjHIYwTQ">here</a>. After installing Ubuntu, run this:</p>
<pre><code class="hljs css language-console"><span class="hljs-meta">$</span><span class="bash"> sudo apt-get update</span>
<span class="hljs-meta">$</span><span class="bash"> sudo apt-get upgrade</span>
<span class="hljs-meta">$</span><span class="bash"> sudo apt-get install python3.6-dev python3-pip git</span>
</code></pre>
<h2><a class="anchor" aria-hidden="true" id="step-2-training-the-handwritten-text-recognition-model"></a><a href="#step-2-training-the-handwritten-text-recognition-model" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Step 2, Training the Handwritten Text Recognition Model:</h2>
<p>We first need to train a model that can recognize handwritten text. We will be using Tensorflow 2.0 and Google Colab for training. In terms of data, we will use the <a href="http://www.fki.inf.unibe.ch/databases/iam-handwriting-database">IAM Database</a>. This data set comes with more than 9,000 pre-labeled text lines from 500 different writers.</p>
<p>Here is an example from the database:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/database-example.jpg" alt="database-example Andrew Bernas"></p>
<p>To access to the database you gave to register <a href="http://www.fki.inf.unibe.ch/DBs/iamDB/iLogin/index.php">here</a>. We can now download all necessary files.</p>
<p>First clone the <a href="https://github.com/bandofpv/Handwritten_Text">Training GitHub Repository</a> in your home folder of your computer:</p>
<pre><code class="hljs css language-console"><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> ~</span>
<span class="hljs-meta">$</span><span class="bash"> git <span class="hljs-built_in">clone</span> https://github.com/bandofpv/Handwritten_Text.git</span>
</code></pre>
<p>Next, we need to setup a virtual environment and install the required python modules:</p>
<pre><code class="hljs css language-console"><span class="hljs-meta">$</span><span class="bash"> sudo pip3 install virtualenv virtualenvwrapper</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"n# virtualenv and virtualenvwrapper"</span> &gt;&gt; ~/.bashrc</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">echo</span> <span class="hljs-string">"export WORKON_HOME=<span class="hljs-variable">$HOME</span>/.virtualenvs"</span> &gt;&gt; ~/.bashrc</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">echo</span> <span class="hljs-string">"export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3"</span> &gt;&gt; ~/.bashrc</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">echo</span> <span class="hljs-string">"source /usr/local/bin/virtualenvwrapper.sh"</span> &gt;&gt; ~/.bashrc</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">source</span> ~/.bashrc</span>
<span class="hljs-meta">$</span><span class="bash"> mkvirtualenv hand -p python3</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> ~/Handwritten_Text</span>
<span class="hljs-meta">$</span><span class="bash"> pip3 install -r requirements.txt</span>
</code></pre>
<p>Next, we can download the database (<strong>Note:</strong> replace <em>your-username</em> with your username and replace <em>your-password</em> with your password from registering):</p>
<pre><code class="hljs css language-console"><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> ~/Handwritten_Text/raw</span>
<span class="hljs-meta">$</span><span class="bash"> USER_NAME=your-username</span>
<span class="hljs-meta">$</span><span class="bash"> PASSWORD=your-password</span>
<span class="hljs-meta">$</span><span class="bash"> wget --user <span class="hljs-variable">$USER_NAME</span> --password <span class="hljs-variable">$PASSWORD</span> -r -np -nH --cut-dirs=3 -A txt,png -P iam http://www.fki.inf.unibe.ch/DBs/iamDB/data/</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> ~/Handwritten_Text/raw/iam/</span>
<span class="hljs-meta">$</span><span class="bash"> wget http://www.fki.inf.unibe.ch/DBs/iamDB/tasks/largeWriterIndependentTextLineRecognitionTask.zip</span>
<span class="hljs-meta">$</span><span class="bash"> unzip -d largeWriterIndependentTextLineRecognitionTask largeWriterIndependentTextLineRecognitionTask.zip</span>
<span class="hljs-meta">$</span><span class="bash"> rm largeWriterIndependentTextLineRecognitionTask.zip robots.txt</span>
</code></pre>
<p>This is a long process so do something fun and look at memes:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/meme1.jpg" alt="meme1 Andrew Bernas"></p>
<p>After downloading the database, we need to transform it into a HDF5 file:</p>
<pre><code class="hljs css language-console">cd ~/Handwritten_Text/src
python3 main.py --source=iam --transform
</code></pre>
<p>This will create a file named iam.hdf5 in the data directory.</p>
<p>Now, we need to open the <a href="https://colab.research.google.com/github/bandofpv/Handwritten_Text/blob/master/src/training.ipynb">training.ipynb</a> file on Google Colab:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/training-colab.jpg" alt="training-colab Andrew Bernas"></p>
<p>Select the <strong>Copy to Drive</strong> tab in the top left corner of the page.</p>
<p>Then, go onto your Google Drive and find the folder named <strong>Colab Notebooks</strong>. Press the <strong>+ New</strong> button on the left and create a new folder named <strong>handwritten-text</strong>. Go into the new folder you created and press the <strong>+ New</strong> button and select the <strong>Folder upload</strong> option. You will need to upload both the src and data folder from our Handwritten_Text directory. You screen should look like this:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/google-drive.jpg" alt="google-drive Andrew Bernas"></p>
<p>Go back to the <a href="https://colab.research.google.com/github/bandofpv/Handwritten_Text/blob/master/src/training.ipynb">training.ipynb</a> tab and confirm that you are hooked up to a GPU runtime. To check, find the <strong>Runtime</strong> tab near the top left corner of the page and select <strong>Change runtime type</strong>. Customize the settings too look like this:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/notebook-settings.jpg" alt="notebook-settings Andrew Bernas"></p>
<p>Google Colab allows us to take advantage of a Tesla K80 GPU, Xeon CPU, and 13GB of RAM all for free and in a notebook style format for training machine learning models.</p>
<p>To prevent Google Colab from disconnecting to the server, press Ctrl+ Shift + I to open inspector view. Select the <strong>Console</strong> tab and enter this:</p>
<pre><code class="hljs css language-console">function ClickConnect(){
console.log("Working"); 
document.querySelector("colab-toolbar-button#connect").click() 
}

setInterval(ClickConnect,60000)
</code></pre>
<p>Your screen should look like this (<strong>Note:</strong> if you get an error about not able to paste into the console, type &quot;allow pasting&quot; as seen below):</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/allow-pasting-colab-console.jpg" alt="allow-pasting-colab-console Andrew Bernas"></p>
<p>Now let's start training! Simply find the <strong>Runtime</strong> tab near the top left corner of the page and select <strong>Run All</strong>. Follow through each code snippet until you reach step 1.2 where you will have to authorize the notebook to access your Google Drive. Just click the link it provides you and copy &amp; paste the authorization code in the input field.</p>
<p>You can then let it the notebook run until it finishes training.</p>
<p>Take a look at the Predict and Evaluate section too see the results:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/results1.jpg" alt="results1 Andrew Bernas"></p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/results2.jpg" alt="results2 Andrew Bernas"></p>
<p>Now that we are done training, we can enjoy another meme:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/meme2.jpg" alt="meme2 Andrew Bernas"></p>
<h2><a class="anchor" aria-hidden="true" id="step-3-setting-up-the-jetson-nano"></a><a href="#step-3-setting-up-the-jetson-nano" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Step 3, Setting Up The Jetson Nano:</h2>
<p>First, follow NVIDIA's tutorial, <a href="https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#intro">Getting Started With Jetson Nano</a>.</p>
<p>After booting up, open a new terminal and download the necessary packages and python modules (<strong>Note:</strong> this will take a LONG time!):</p>
<pre><code class="hljs css language-console"><span class="hljs-meta">$</span><span class="bash"> sudo apt-get update</span>
<span class="hljs-meta">$</span><span class="bash"> sudo apt-get upgrade</span>
<span class="hljs-meta">$</span><span class="bash"> sudo apt-get install python3-pip gcc-8 g++-8 libopencv-dev</span>
<span class="hljs-meta">$</span><span class="bash"> sudo apt-get install libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev gfortran libopenblas-dev liblapack-dev</span>
<span class="hljs-meta">$</span><span class="bash"> sudo pip3 install -U pip testresources setuptools cython</span>
<span class="hljs-meta">$</span><span class="bash"> sudo pip3 install -U numpy==1.16.1 future==0.17.1 mock==3.0.5 h5py==2.9.0 keras_preprocessing==1.0.5 keras_applications==1.0.8 gast==0.2.2 enum34 futures protobuf</span>
<span class="hljs-meta">$</span><span class="bash"> sudo pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v43 tensorflow-gpu==2.0.0+nv20.1</span>
</code></pre>
<p>This will install OpenCV 4.0 for computer vision, gcc-8 &amp; g++-8 for C++ compiling, and TensorFlow 2.0 to run our handwritten text recognition model. Those are the main ones, but we will also need to install all the other required dependencies:</p>
<pre><code class="hljs css language-console"><span class="hljs-meta">$</span><span class="bash"> sudo pip3 uninstall enum34</span>
<span class="hljs-meta">$</span><span class="bash"> sudo apt-get install python3-matplotlib python3-numpy python3-pil python3-scipy nano</span>
<span class="hljs-meta">$</span><span class="bash"> sudo apt-get install build-essential cython</span>
<span class="hljs-meta">$</span><span class="bash"> sudo apt install --reinstall python*-decorator</span>
<span class="hljs-meta">$</span><span class="bash"> sudo pip3 install -U scikit-image</span>
<span class="hljs-meta">$</span><span class="bash"> sudo pip3 install -U google-cloud-vision google-cloud-texttospeech imutils pytesseract pyttsx3 natsort playsound </span>
<span class="hljs-meta">$</span><span class="bash"> sudo pip3 install -U autopep8==1.4.4 editdistance==0.5.3 flake8==3.7.9 kaldiio==2.15.1</span>
</code></pre>
<p>We also need to install llvmlite from source in order to install numba (<strong>Note:</strong> This will take an even LONGER time. Treat your self to a nice movie!):</p>
<pre><code class="hljs css language-console"><span class="hljs-meta">$</span><span class="bash"> wget http://releases.llvm.org/7.0.1/llvm-7.0.1.src.tar.xz</span>
<span class="hljs-meta">$</span><span class="bash"> tar -xvf llvm-7.0.1.src.tar.xz</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> llvm-7.0.1.src</span>
<span class="hljs-meta">$</span><span class="bash"> mkdir llvm_build_dir</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> llvm_build_dir/</span>
<span class="hljs-meta">$</span><span class="bash"> cmake ../ -DCMAKE_BUILD_TYPE=Release -DLLVM_TARGETS_TO_BUILD=<span class="hljs-string">"ARM;X86;AArch64"</span></span>
<span class="hljs-meta">$</span><span class="bash"> make -j4</span>
<span class="hljs-meta">$</span><span class="bash"> sudo make install</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> bin/</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">echo</span> <span class="hljs-string">"export LLVM_CONFIG=\""</span>`<span class="hljs-built_in">pwd</span>`<span class="hljs-string">"/llvm-config\""</span> &gt;&gt; ~/.bashrc</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">echo</span> <span class="hljs-string">"alias llvm='"</span>`<span class="hljs-built_in">pwd</span>`<span class="hljs-string">"/llvm-lit'"</span> &gt;&gt; ~/.bashrc</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">source</span> ~/.bashrc</span>
<span class="hljs-meta">$</span><span class="bash"> sudo pip3 install -U llvmlite numba</span>
</code></pre>
<p>Next, you need to plug in the WiFi USB adapter into any of the USB ports:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/wifi-usb.jpg" alt="wifi-usb Andrew Bernas"></p>
<p>You then need to turn off power save mode to prevent the WiFi network from dropping out:</p>
<pre><code class="hljs css language-console">sudo iw dev wlan0 set power_save off
</code></pre>
<p>In order to take advantage of Google Cloud's Vision and Text-to-Speech APIs, we need to create an <a href="https://cloud.google.com/">account</a>. It will require you to enter your credit card, but don't worry, you get $300 free! Next go to your <a href="https://console.cloud.google.com/">GCP Console</a> and create a new project with any name using the following steps.</p>
<ol>
<li>Go to the <a href="https://console.cloud.google.com/apis/library?project=_">Cloud Console API Library</a> and click on <strong>Select your project</strong>.</li>
<li>On the search bar enter &quot;Vision&quot;.</li>
<li>Select <strong>Cloud Vision API</strong> and click <strong>Enable</strong>.</li>
<li>Go back to the <a href="https://console.cloud.google.com/apis/library?project=_">Cloud Console API Library</a> and enter &quot;Text to Speech&quot; in the search bar.</li>
<li>Select &quot;Cloud Text-to-Speech API&quot; and click <strong>Enable</strong>.</li>
<li>On the <a href="https://console.cloud.google.com/iam-admin/serviceaccounts/">Google Cloud Service accounts page</a> click on <strong>Select your project</strong> and select <strong>Create Service Account</strong>.</li>
<li>Designate the <strong>Service account name</strong> and click <strong>Create</strong>.</li>
<li>Under <strong>Service account permissions</strong> click <strong>Select a role</strong>.</li>
<li>Scroll to <em>Cloud Translation</em> and select <em>Cloud Translation API Editor</em>. Select <strong>Continue</strong>.</li>
<li>Click <strong>Create Key</strong>, select <strong>JSON</strong>, and click <strong>Create</strong>.</li>
</ol>
<p>This will create a .json file that will allow your Jetson Nano to connect to your cloud project.</p>
<p>We also need to check the name of our Google Cloud Project ID. Go to your <a href="https://console.cloud.google.com/">GCP Console</a> on the <strong>Project info</strong> section, you will see <em>ProjectID</em>. This is your Project ID. We will use it soon.</p>
<p>Now we can clone the <a href="https://github.com/bandofpv/Reading_Eye_For_The_Blind">Reading_Eye_For_The_Blind GitHub Repository</a>:</p>
<pre><code class="hljs css language-console"><span class="hljs-meta">$</span><span class="bash"> git <span class="hljs-built_in">clone</span> https://github.com/bandofpv/Reading_Eye_For_The_Blind.git</span>
</code></pre>
<p>Remember the handwritten text recognition model we trained earlier?
Now we need to save it into our Reading_Eye_For_The_Blind directory.</p>
<p>Go back to the <em>handwritten-text</em> Google Drive folder:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/google-drive2.jpg" alt="google-drive2 Andrew Bernas"></p>
<p>Right click on the output folder and click &quot;Download&quot;. This is our model that we trained. Move it to our directory and rename it (<strong>Note</strong>: Change <em>name-of-dowloaded-zip-file</em> to the name of the downloaded zip file):</p>
<pre><code class="hljs css language-console"><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">export</span> YOUR_DOWNLOAD=name-of-dowloaded-zip-file</span>
<span class="hljs-meta">$</span><span class="bash"> unzip ~/Downloads/<span class="hljs-variable">$YOUR_DOWNLOAD</span> -d ~/Reading_Eye_For_The_Blind</span>
<span class="hljs-meta">$</span><span class="bash"> mv ~/Reading_Eye_For_The_Blind/output ~/Reading_Eye_For_The_Blind/model</span>
</code></pre>
<p>To make the our program run on the Jetson Nano every time we turn it on we need to create an <em>autostart</em> directory:</p>
<pre><code class="hljs css language-console"><span class="hljs-meta">$</span><span class="bash"> mkdir ~/.config/autostart</span>
</code></pre>
<p>We need to move <a href="https://github.com/bandofpv/Reading_Eye_For_The_Blind/blob/master/ReadingEye.desktop">ReadingEye.desktop</a> into our new directory:</p>
<pre><code class="hljs css language-console"><span class="hljs-meta">$</span><span class="bash"> mv ~/Reading_Eye_For_The_Blind/ReadingEye.desktop ~/.config/autostart/</span>
</code></pre>
<p>We also need to update the environmental variables in our file:</p>
<pre><code class="hljs css language-console">nano ~/.config/autostart/ReadingEye.desktop
</code></pre>
<p>You then want to find USERNAME and replace it with the username of your Jetson Nano (<strong>Note</strong>: There are two occurrences of USERNAME, change both!).</p>
<p>Next we need to connect a camera to the Jetson Nano. I used a <a href="https://www.amazon.com/Raspberry-Pi-Camera-Module-Megapixel/dp/B01ER2SKFS/ref=pd_bxgy_2/137-9852188-5881617?_encoding=UTF8&amp;pd_rd_i=B01ER2SKFS&amp;pd_rd_r=84410404-fc53-4886-9e88-36bf54ffac8e&amp;pd_rd_w=Cm26g&amp;pd_rd_wg=goybO&amp;pf_rd_p=fd08095f-55ff-4a15-9b49-4a1a719">Raspberry Pi Camera Module V2</a> with a <a href="https://www.amazon.com/Arducam-Replacement-Raspberry-Degrees-Diagonal/dp/B07V322VCX?th=1">wide angle lens attachment</a>.</p>
<p>Simply remove the original lens and attach the wide angle one. It should look like this:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/rpi-camera.jpg" alt="rpi-camera Andrew Bernas"></p>
<p>Connect the ribbon cable to the Jetson Nano's camera connector (<strong>Note</strong>: Make sure to use a <a href="https://www.adafruit.com/product/1730">longer ribbon cable</a> than the original one provided. Also make sure to connect the ribbon cable in the correct orientation):</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/ribbon-cable.jpg" alt="ribbon-cable Andrew Bernas"></p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/ribbon-cable2.jpg" alt="ribbon-cable2 Andrew Bernas"></p>
<p>Install a fan to the Jetson Nano to prevent it from throttling during heavy computing. We will use a <a href="https://www.amazon.com/Noctua-NF-A4x20-5V-PWM-Premium-Quality/dp/B071FNHVXN/ref=as_li_ss_tl?crid=27DIIHQ63FHYV&amp;keywords=noctua+a4x20+5v+pwm&amp;qid=1567903014&amp;s=gateway&amp;sprefix=noctua+A4,aps,390&amp;sr=8-1-spons&amp;psc=1&amp;spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUExTFgwSUo2VERR">Noctua 5V Fan</a>, which is extremely quiet and efficient. Use <a href="https://www.getfpv.com/m3x25-12-9-grade-steel-screw-set-50pcs.html">M3x25 screws</a> to mount the fan into the mounting holes, then plug the cable into the fan header:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/fan.jpg" alt="fan Andrew Bernas"></p>
<p>We can start soldering the power switch and start button. The power switch is a toggle switch with a LED light. Solder female jumper wires to the three pins (<strong>Note</strong>: Pay close attention to the pin labels! I accidentally swapped the ground and light pins as seen in the photos. The pin sticking on the side is the <strong><em>ground</em></strong>, the pin with a <strong>+</strong> is the <strong><em>signal pin</em></strong>, and the pin with a strange light symbol is the <em>light pin</em>):</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/power-switch.jpg" alt="power-switch Andrew Bernas"></p>
<p>For the start button it can be more complicated because we will have to setup a pull up resister circuit with the button. Here is a schematic:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/pull-up.jpg" alt="pull-up Andrew Bernas"></p>
<p>Solder a <em>ground</em> jumper wire to the <strong>-</strong> pin, a <em>signal</em> jumper wire to the <strong>+</strong> pin, and a <em>10k resistor</em> to the <strong>+</strong> pin on the start button too. Next, solder a <em>power</em> jumper wire to the resistor. This will plug into the 3.3V pin on the Jetson Nano.</p>
<p>We also need to solder the LED pins on the start button too. Just solder a power jumper wire to the <strong>+</strong> light pin and a <em>ground jumper</em> wire to the <strong>-</strong> light pin. This is how mine looks like:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/button1.jpg" alt="button1 Andrew Bernas"></p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/button2.jpg" alt="button2 Andrew Bernas"></p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/button3.jpg" alt="button3 Andrew Bernas"></p>
<p>We can now connect the power switch and start button to the correct pins. Here is a schematic:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/schematic.jpg" alt="schematic Andrew Bernas"></p>
<p>Note the jumper on the top of the J40 header.</p>
<p>Great job! Enjoy a meme:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/meme3.jpg" alt="meme3 Andrew Bernas"></p>
<h2><a class="anchor" aria-hidden="true" id="step-4-building-the-enclosure"></a><a href="#step-4-building-the-enclosure" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Step 4, Building The Enclosure:</h2>
<p>3D print the CAD files included at the <a href="https://www.hackster.io/bandofpv/reading-eye-for-the-blind-with-jetson-nano-8657ed#cad">bottom of this tutorial</a>. We need to fit all our electronics into this box. I used double sided foam tape to mount the the power pack to the bottom. You want to make sure it is orientated properly:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/enclosure1.jpg" alt="enclosure1 Andrew Bernas"></p>
<p>Next, we need to mount the Jetson Nano on top of the battery pack and connect the USB cable between the two:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/enclosure2.jpg" alt="enclosure2 Andrew Bernas"></p>
<p>Plug in the USB to audio audio adapter and hot glue it to the side hole:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/enclosure3.jpg" alt="enclosure3 Andrew Bernas"></p>
<p>Next, mount the power toggle switch and start button to the lid and plug it into the correct ports on the Jetson Nano:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/enclosure4.jpg" alt="enclosure4 Andrew Bernas"></p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/enclosure5.jpg" alt="enclosure5 Andrew Bernas"></p>
<p>Finally, glue the lid to the box using super glue. Make sure that the camera is outside the box mount it to the camera mount using hot glue. You can also do intricate ribbon cable folding to keep it aesthetic:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/enclosure6.jpg" alt="enclosure4 Andrew Bernas"></p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/enclosure7.jpg" alt="enclosure5 Andrew Bernas"></p>
<h2><a class="anchor" aria-hidden="true" id="step-5-have-fun"></a><a href="#step-5-have-fun" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Step 5, Have Fun:</h2>
<p>Now that we're done with the build have fun and start detecting! Plug in your favorite pair of earphones to the audio jack and place some text under the camera, toggle the power switch up and down, and click the start button.</p>
<p>To detect printed text, press the start button once. To detect handwritten text, double click the start button.</p>
<h2><a class="anchor" aria-hidden="true" id="understanding-the-code"></a><a href="#understanding-the-code" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Understanding The Code:</h2>
<p>Now for the even more fun part. Code!!!</p>
<p>All code can be found on both GitHub Repositories:</p>
<ul>
<li><a href="https://github.com/bandofpv/Reading_Eye_For_The_Blind">Reading_Eye_For_The_Blind GitHub Repository</a></li>
<li><a href="https://github.com/bandofpv/Handwritten_Text">Handwritten_Text GitHub Repository</a></li>
</ul>
<p>There is a lot of code used in this tutorial and I will explain the fundamentals. My explanations will be very broad, so if you want to learn more about how my code works, take a look at my GitHub Repositories above or ask me in the comment section.</p>
<p>Lets start with a flow chart:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/flow-chart.jpg" alt="flow-chart Andrew Bernas"></p>
<p>This will help understand the basics of what goes on. I decided to tryout Google's Cloud Vision and TTS API in this project. It allows you to use cloud computing to perform both printed and handwritten text recognition and text to speech. However, because the purpose of the Jetson Nano is to promote AI at the Edge, I programmed it to perform both printed and handwritten text recognition and text to speech without internet connection as well.</p>
<p>Let's go into each step in depth.</p>
<p>The first step is to check if the Jetson Nano is connected to internet:</p>
<pre><code class="hljs css language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">check_internet</span><span class="hljs-params">(host=<span class="hljs-string">'http://google.com'</span>)</span>:</span>
    <span class="hljs-keyword">try</span>:
        urllib.request.urlopen(host)
        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>
    <span class="hljs-keyword">except</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>
</code></pre>
<p>By using the urllib module, we can attempt to connect to <a href="https://www.google.com/">google.com</a>. If it can connect to Google, then it returns that it has internet connection. If not, then it will return that it does not have internet connection.</p>
<p>After checking if the Jetson Nano has internet connection, it will take a picture using OpenCV:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/page1.jpg" alt="page1 Andrew Bernas"></p>
<p>But as seen above, the picture has this <em>fisheye</em> effect due to the wide angle lens. To correct for this, we can use OpenCV again to get a much better picture:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/page2.jpg" alt="page2 Andrew Bernas"></p>
<p>Next, if the Jetson Nano has internet connection, it will connect to the Google Cloud and use the Vision and Text-to-Speech APIs to recognize both printed and handwritten text and synthesize it to speech.</p>
<p>However, if the Jetson Nano does not have internet connection, it will have to rely on its CPU and GPU to recognize printed and handwritten text and synthesize it to speech.</p>
<p>Let's start with printed text because its easiest to understand. The Jetson Nano will first attempt to detect a page in the photo. This is done using OpenCV's edge detection:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/page3.jpg" alt="page3 Andrew Bernas"></p>
<p>If it detects a page, then it will transform it into a top-down view:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/page4.jpg" alt="page4 Andrew Bernas"></p>
<p>However, if the Jetson Nano does not detect a page, it will think its detecting text from a book. In this case, it will just convert it into grayscale and perform adaptive thresholding to remove shadows:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/page5.jpg" alt="page5 Andrew Bernas"></p>
<p>After preprocessing the image into a picture ready for printed text recognition, we will use PyTesseract:</p>
<pre><code class="hljs css language-python">text = pytesseract.image_to_string(img)
</code></pre>
<p>Next, we will use pyttsx3 to synthesize the recognized text into speech:</p>
<pre><code class="hljs css language-python">engine = pyttsx3.init()
engine.setProperty(<span class="hljs-string">'voice'</span>, <span class="hljs-string">"en-us+f5"</span>)
engine.say(text)
</code></pre>
<p>Now for the handwritten text. Handwritten text is much harder to detect then printed text because unlike typed text, handwriting is never perfectly legible and requires the human brain to infer what word you are reading. This gives the Jetson Nano an even harder time to recognized handwritten text.</p>
<p>At the <a href="https://www.hackster.io/bandofpv/reading-eye-for-the-blind-with-jetson-nano-8657ed#toc-step-2--training-the-handwritten-text-recognition-model-3">beginning of this tutorial</a>, we trained our own handwritten text recognition model using Google Colab. I will briefly explain how this works.</p>
<p>Using the <a href="http://www.fki.inf.unibe.ch/databases/iam-handwriting-database">IAM Database</a>, with more than 9,000 pre-labeled text lines from 500 different writers, we trained a handwritten text recognition:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/iam-database.jpg" alt="iam-database Andrew Bernas"></p>
<p>Through Deep Learning, we fit our data set into the Convolutional Recurrent Neural Network (CRNN) which can overcome some limitations of a traditional Hidden Markov Model (HMM):</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/crnn.jpg" alt="crnn Andrew Bernas"></p>
<p>First, the input image is fed through the Convolutional Neural Network (CNN) layers which will extract the relevant features from the image. Each layer consists of three operations:</p>
<ol>
<li>Convolution Operation</li>
<li>Non-Linear RELU Function</li>
<li>Pooling</li>
</ol>
<p>Next, the output feature map from the CNN layers will be fed into the Recurrent Neural Network (RNN) which will propagate relevant information through longer distances, providing more robust training.</p>
<p>Finally, the Connectionist Temporal Classification (CTC) will calculate loss value and decodes the RNN output sequence into the final text.</p>
<p>Below is an image that goes into more detail of the RNN layers.</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/crnn2.jpg" alt="crnn2 Andrew Bernas"></p>
<p>That's the most detail I will include in this tutorial because its relatively complicated and too long to add a full description in this tutorial. Again, ask in the comments if you have any questions.</p>
<p>Now that you somewhat understand the basics of our handwritten text recognition model, you could also notice that it can only detect words on one single line. This is where text segmentation comes to play.</p>
<p>The text segmentation process starts just like the printed text. It attempts to find a page and perform four-point perspective transformation:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/transform.jpg" alt="transform Andrew Bernas"></p>
<p>Then comes the binarization. There are a few techniques to perform this:</p>
<ul>
<li>A Threshold Selection Method from Gray-Level Histograms (Otsu)</li>
<li>An Introduction to Digital Image Processing (Niblack)</li>
<li>Adaptive Document Binarization (Sauvola)</li>
<li>Text Localization, Enhancement, and Binarization in Multimedia Documents (Wolf)</li>
<li>Binarization of Historical Document Images Using the Local Maximum and Minimum (Su)</li>
</ul>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/binarization.jpg" alt="binarization Andrew Bernas"></p>
<p>You may notice that the images are still quite distorted due to shadows. This is where Illumination compensation will come to the rescue:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/illumination.jpg" alt="illumination Andrew Bernas"></p>
<p>Notice the dramatic improvements!</p>
<p>By coupling the two techniques we get this:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/binarization-and-illumination.jpg" alt="binarization-and-illumination Andrew Bernas"></p>
<p>Sauvola is used in the code because it has the most consistent results.</p>
<p>After all this preprocessing, we can now begin the text segmentation. There are several levels of text segmentation: line, word, and character. In our program, we want to do line segmentation because that is what our CRNN is trained on (I will get into more detail about this in the <a href="https://www.hackster.io/bandofpv/reading-eye-for-the-blind-with-jetson-nano-tensorflow-8657ed#toc-next-steps-8">Next Steps</a> section). There are also a wide variety of methodologies when approaching text segmentation (pixel counting approach, histogram approach, Y histogram projection, text line separation, false line exclusion, line region recovery, smearing approach, stochastic approach, water flow approach, and so on). In our program, we used a statistical and path planning approach to line segmentation in handwritten documents</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/line-segmentation.jpg" alt="line-segmentation Andrew Bernas"></p>
<p>To further preprocess the text for handwritten text recognition, deslanting is also used:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/deslanting.jpg" alt="deslanting Andrew Bernas"></p>
<p>This will help remove the cursive writing style and help improve the text recognition results.</p>
<p>Pairing up our segmented text lines and our handwritten text recognition model, we can successfully recognize handwritten text and synthesize it into speech!</p>
<h2><a class="anchor" aria-hidden="true" id="next-steps"></a><a href="#next-steps" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Next Steps:</h2>
<p>Upon the completion of this project, I have thought of future revisions:</p>
<h3><a class="anchor" aria-hidden="true" id="1-smaller-enclosure"></a><a href="#1-smaller-enclosure" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>1. Smaller Enclosure:</h3>
<p>The box I designed turned out a lot bigger than I expected:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/next-steps1.jpg" alt="next-steps1 Andrew Bernas"></p>
<p>I wanted to make sure that I would have enough space to fit everything inside. After building the box, I realized that I can create a much smaller enclosure.</p>
<h3><a class="anchor" aria-hidden="true" id="2-ctc-word-beamsearch"></a><a href="#2-ctc-word-beamsearch" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>2. CTC Word BeamSearch:</h3>
<p>Remember that Connectionist Temporal Classification (CTC) step in our handwritten text recognition training I explained earlier?</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/next-steps2.jpg" alt="next-steps2 Andrew Bernas"></p>
<p>Well, I found a <a href="https://github.com/githubharald/CTCWordBeamSearch">Word Beam Search CTC Algorithm</a> that can greatly improve the final word decoding:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/next-steps3.jpg" alt="next-steps3 Andrew Bernas"></p>
<p>The algorithm is able to recognize words by using a dictionary. Doing so, It is able to always get an actual word rather than some conglomeration of characters.</p>
<h3><a class="anchor" aria-hidden="true" id="3-more-data"></a><a href="#3-more-data" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>3. More Data:</h3>
<p>In this tutorial, we used the <a href="http://www.fki.inf.unibe.ch/databases/iam-handwriting-database">IAM Database</a>. While it does come with more than 9, 000 pre-labeled text lines from 500 different writers, I often find the robot creating errors in handwritten text recognition. I have found other databases such as <a href="http://transcriptorium.eu/datasets/bentham-collection/">Bentham</a> and <a href="http://www.a2ialab.com/doku.php?id=rimes_database:start">Rimes</a> which would provide our model with a greater variety of handwritings to train on. I could even start collecting my own samples to help improve the accuracy of the handwritten text recognition.</p>
<h3><a class="anchor" aria-hidden="true" id="4-more-languages"></a><a href="#4-more-languages" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>4. More Languages:</h3>
<p>The robot can only recognize and speak English. I want to include several other languages such as Hindi, Mandarin, Russian, and Spanish in the future.</p>
</span></div></article></div><div class="docs-prevnext"><a class="docs-prev button" href="/docs/tutorials/sar-drone"><span class="arrow-prev">← </span><span>Search And Rescue Drone</span></a><a class="docs-next button" href="/docs/tutorials/recycle-sorting-robot"><span>Recycle Sorting Robot</span><span class="arrow-next"> →</span></a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#story">Story:</a></li><li><a href="#demo-video">Demo Video:</a></li><li><a href="#code">Code:</a></li><li><a href="#hardware-components">Hardware components:</a></li><li><a href="#hand-tools-and-fabrication-machines">Hand tools and fabrication machines:</a></li><li><a href="#3d-files">3D Files:</a></li><li><a href="#step-1-setting-up-your-computer">Step 1, Setting Up Your Computer:</a></li><li><a href="#step-2-training-the-handwritten-text-recognition-model">Step 2, Training the Handwritten Text Recognition Model:</a></li><li><a href="#step-3-setting-up-the-jetson-nano">Step 3, Setting Up The Jetson Nano:</a></li><li><a href="#step-4-building-the-enclosure">Step 4, Building The Enclosure:</a></li><li><a href="#step-5-have-fun">Step 5, Have Fun:</a></li><li><a href="#understanding-the-code">Understanding The Code:</a></li><li><a href="#next-steps">Next Steps:</a><ul class="toc-headings"><li><a href="#1-smaller-enclosure">1. Smaller Enclosure:</a></li><li><a href="#2-ctc-word-beamsearch">2. CTC Word BeamSearch:</a></li><li><a href="#3-more-data">3. More Data:</a></li><li><a href="#4-more-languages">4. More Languages:</a></li></ul></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="/" class="nav-home"><img src="/img/FullSizeRender.jpg" alt="Andrew Bernas" width="66" height="58"/></a><div><h5>Projects</h5><a href="/docs/in-progress/sar-drone.html">In Progress</a><a href="/docs/incomplete-prototypes/lockheed-vega.html">Incomplete Prototypes</a><a href="/docs/robots/recycle-sorting-robot.html">Robots</a><a href="/docs/drones/qav-r.html">Drones</a><a href="/docs/rc-airplanes/explorer.html">RC Airplanes</a><a href="/docs/conservation-service-projects/fish-and-wildlife-management.html">Conservation Service Projects</a></div><div><h5>More</h5><a href="/docs/tutorials/sar-drone.html">Tutorials</a><a href="/gallery.html">Photography</a></div><div><h5>Profile</h5><a href="/about-me.html">About Me</a><a href="https://docs.google.com/document/d/1LFxUAfSPOlgA_n8IyqA-czno1DAukBsEL19MBdMJNjs/edit?usp=sharing" target="_blank" rel="noreferrer noopener">Resume</a><a href="https://www.youtube.com/channel/UCYIknwUG33u7_Se2__GrHrg" target="_blank" rel="noreferrer noopener">Youtube</a><a href="https://github.com/bandofpv" target="_blank" rel="noreferrer noopener">GitHub</a></div></section><section class="copyright">Copyright © 2021 Andrew Bernas</section></footer></div></body></html>