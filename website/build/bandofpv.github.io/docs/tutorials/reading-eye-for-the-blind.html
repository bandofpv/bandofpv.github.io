<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Reading Eye For The Blind With NVIDIA Jetson Nano · Andrew Bernas</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="December 2019 - February 2020"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Reading Eye For The Blind With NVIDIA Jetson Nano · Andrew Bernas"/><meta property="og:type" content="website"/><meta property="og:url" content="https://bandofpv.github.io/"/><meta property="og:description" content="December 2019 - February 2020"/><meta property="og:image" content="https://bandofpv.github.io/img/favicon.ico"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://bandofpv.github.io/img/favicon.ico"/><link rel="shortcut icon" href="/img/favicon.ico"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/favicon.ico" alt="Andrew Bernas"/><h2 class="headerTitleWithLogo">Andrew Bernas</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/about-me" target="_self">About Me</a></li><li class=""><a href="/docs/in-progress/lockheed-vega" target="_self">Projects</a></li><li class="siteNavGroupActive siteNavItemActive"><a href="/docs/tutorials/reading-eye-for-the-blind" target="_self">Tutorials</a></li><li class=""><a href="https://www.youtube.com/channel/UCYIknwUG33u7_Se2__GrHrg" target="_self">Youtube</a></li><li class=""><a href="https://github.com/bandofpv" target="_self">GitHub</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>Tutorials</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle collapsible">Tutorials<span class="arrow"><svg width="24" height="24" viewBox="0 0 24 24"><path fill="#565656" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path d="M0 0h24v24H0z" fill="none"></path></svg></span></h3><ul class="hide"><li class="navListItem navListItemActive"><a class="navItem" href="/docs/tutorials/reading-eye-for-the-blind">Reading Eye For The Blind With NVIDIA Jetson Nano</a></li><li class="navListItem"><a class="navItem" href="/docs/tutorials/ft-explorer-vtol-tutorial">FT Explorer VTOL Build</a></li><li class="navListItem"><a class="navItem" href="/docs/tutorials/qav-r-tutorial">QAV-R Build</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer docsContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1 id="__docusaurus" class="postHeaderTitle">Reading Eye For The Blind With NVIDIA Jetson Nano</h1></header><article><div><span><p>December 2019 - February 2020</p>
<h2><a class="anchor" aria-hidden="true" id="story"></a><a href="#story" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Story</h2>
<p>The World Health Organization (WHO) estimates that there are 285 million people that are visually impaired. Specifically in the United States, there are 1.3 million who are legally blind and only 10 percent of them can actually read braille and only 10 percent of the blind children are learning it. This is due to how expensive books are to read in braille. It costs up to $15,000 just to convert five chapters of a science book! Due to the price, very few blind people are able to learn through books. People who are reading impaired or suffer vision loss also struggle to read. While many can use audio-books, they are still limited on what they can read based on audio books' availability and costs. Books are the cheapest way to learn and 285 million people are unable to take advantage of this resource we greatly take for granted. The Reading Eye device would allow more freedom in terms of book choice, without having to make investments towards buying several audio-books. It is able to detect printed and handwritten text and speak it in a realistic synthesized voice.</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/story.jpg" alt="story"></p>
<p>My whole inspiration for this project was to help my grandmother who's vision degrades everyday due to age. I then thought of all the others who suffer due to bad vision or reading disabilities which motivated me to pursue this project.</p>
<h3><a class="anchor" aria-hidden="true" id="demo-video"></a><a href="#demo-video" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Demo Video:</h3>
<p><a href="https://www.youtube.com/watch?v=ZVquCjLMWuA"><img src="/docs/assets/tutorials/reading-eye-for-the-blind/demo-video.jpg" alt="demo-video"></a></p>
<h3><a class="anchor" aria-hidden="true" id="code"></a><a href="#code" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Code:</h3>
<ul>
<li><strong><a href="https://github.com/bandofpv/Reading_Eye_For_The_Blind">Reading_Eye_For_The_Blind GitHub Repository</a></strong></li>
<li><strong><a href="https://github.com/bandofpv/Handwritten_Text">Handwritten_Text GitHub Repository</a></strong></li>
</ul>
<p><strong>Note:</strong> When I refer to &quot;your computer&quot; I am referring to a Ubuntu 18.04 LTS machine. When I refer to &quot;your Jetson Nano&quot; I am referring to your Jetson Nano. While this tutorial can all be done using a Jetson Nano, I would not recommend it because it is slow during heavy processing compared to a traditional desktop machine.</p>
<h3><a class="anchor" aria-hidden="true" id="step-1-setting-up-your-computer"></a><a href="#step-1-setting-up-your-computer" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Step 1, Setting Up Your Computer:</h3>
<p>As stated above, I recommend a clean install of Ubuntu 18.04 LTS. Instructions on installing Ubuntu can be found <a href="https://www.youtube.com/watch?v=u5QyjHIYwTQ">here</a>. After installing Ubuntu, run this:</p>
<pre><code class="hljs css language-console"><span class="hljs-meta">$</span><span class="bash"> sudo apt-get update</span>
<span class="hljs-meta">$</span><span class="bash"> sudo apt-get upgrade</span>
<span class="hljs-meta">$</span><span class="bash"> sudo apt-get install python3.6-dev python3-pip git</span>
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="step-2-training-the-handwritten-text-recognition-model"></a><a href="#step-2-training-the-handwritten-text-recognition-model" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Step 2, Training the Handwritten Text Recognition Model:</h3>
<p>We first need to train a model that can recognize handwritten text. We will be using Tensorflow 2.0 and Google Colab for training. In terms of data, we will use the <a href="http://www.fki.inf.unibe.ch/databases/iam-handwriting-database">IAM Database</a>. This data set comes with more than 9,000 pre-labeled text lines from 500 different writers.</p>
<p>Here is an example from the database:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/database-example.jpg" alt="database-example"></p>
<p>To access to the database you gave to register <a href="http://www.fki.inf.unibe.ch/DBs/iamDB/iLogin/index.php">here</a>. We can now download all necessary files.</p>
<p>First clone the <a href="https://github.com/bandofpv/Handwritten_Text">Training GitHub Repository</a> in your home folder of your computer:</p>
<pre><code class="hljs css language-console"><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> ~</span>
<span class="hljs-meta">$</span><span class="bash"> git <span class="hljs-built_in">clone</span> https://github.com/bandofpv/Handwritten_Text.git</span>
</code></pre>
<p>Next, we need to setup a virtual environment and install the required python modules:</p>
<pre><code class="hljs css language-console"><span class="hljs-meta">$</span><span class="bash"> sudo pip3 install virtualenv virtualenvwrapper</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"n# virtualenv and virtualenvwrapper"</span> &gt;&gt; ~/.bashrc</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">echo</span> <span class="hljs-string">"export WORKON_HOME=<span class="hljs-variable">$HOME</span>/.virtualenvs"</span> &gt;&gt; ~/.bashrc</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">echo</span> <span class="hljs-string">"export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3"</span> &gt;&gt; ~/.bashrc</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">echo</span> <span class="hljs-string">"source /usr/local/bin/virtualenvwrapper.sh"</span> &gt;&gt; ~/.bashrc</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">source</span> ~/.bashrc</span>
<span class="hljs-meta">$</span><span class="bash"> mkvirtualenv hand -p python3</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> ~/Handwritten_Text</span>
<span class="hljs-meta">$</span><span class="bash"> pip3 install -r requirements.txt</span>
</code></pre>
<p>Next, we can download the database (<strong>Note:</strong> replace <em>your-username</em> with your username and replace <em>your-password</em> with your password from registering):</p>
<pre><code class="hljs css language-console"><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> ~/Handwritten_Text/raw</span>
<span class="hljs-meta">$</span><span class="bash"> USER_NAME=your-username</span>
<span class="hljs-meta">$</span><span class="bash"> PASSWORD=your-password</span>
<span class="hljs-meta">$</span><span class="bash"> wget --user <span class="hljs-variable">$USER_NAME</span> --password <span class="hljs-variable">$PASSWORD</span> -r -np -nH --cut-dirs=3 -A txt,png -P iam http://www.fki.inf.unibe.ch/DBs/iamDB/data/</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> ~/Handwritten_Text/raw/iam/</span>
<span class="hljs-meta">$</span><span class="bash"> wget http://www.fki.inf.unibe.ch/DBs/iamDB/tasks/largeWriterIndependentTextLineRecognitionTask.zip</span>
<span class="hljs-meta">$</span><span class="bash"> unzip -d largeWriterIndependentTextLineRecognitionTask largeWriterIndependentTextLineRecognitionTask.zip</span>
<span class="hljs-meta">$</span><span class="bash"> rm largeWriterIndependentTextLineRecognitionTask.zip robots.txt</span>
</code></pre>
<p>This is a long process so do something fun and look at memes:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/meme1.jpg" alt="meme1"></p>
<p>After downloading the database, we need to transform it into a HDF5 file:</p>
<pre><code class="hljs css language-console">cd ~/Handwritten_Text/src
python3 main.py --source=iam --transform
</code></pre>
<p>This will create a file named iam.hdf5 in the data directory.</p>
<p>Now, we need to open the <a href="https://colab.research.google.com/github/bandofpv/Handwritten_Text/blob/master/src/training.ipynb">training.ipynb</a> file on Google Colab:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/training-colab.jpg" alt="training-colab"></p>
<p>Select the <strong>Copy to Drive</strong> tab in the top left corner of the page.</p>
<p>Then, go onto your Google Drive and find the folder named <strong>Colab Notebooks</strong>. Press the <strong>+ New</strong> button on the left and create a new folder named <strong>handwritten-text</strong>. Go into the new folder you created and press the <strong>+ New</strong> button and select the <strong>Folder upload</strong> option. You will need to upload both the src and data folder from our Handwritten_Text directory. You screen should look like this:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/google-drive.jpg" alt="google-drive"></p>
<p>Go back to the <a href="https://colab.research.google.com/github/bandofpv/Handwritten_Text/blob/master/src/training.ipynb">training.ipynb</a> tab and confirm that you are hooked up to a GPU runtime. To check, find the <strong>Runtime</strong> tab near the top left corner of the page and select <strong>Change runtime type</strong>. Customize the settings too look like this:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/notebook-settings.jpg" alt="notebook-settings"></p>
<p>Google Colab allows us to take advantage of a Tesla K80 GPU, Xeon CPU, and 13GB of RAM all for free and in a notebook style format for training machine learning models.</p>
<p>To prevent Google Colab from disconnecting to the server, press Ctrl+ Shift + I to open inspector view. Select the <strong>Console</strong> tab and enter this:</p>
<pre><code class="hljs css language-console">function ClickConnect(){
console.log("Working"); 
document.querySelector("colab-toolbar-button#connect").click() 
}

setInterval(ClickConnect,60000)
</code></pre>
<p>Your screen should look like this (<strong>Note:</strong> if you get an error about not able to paste into the console, type &quot;allow pasting&quot; as seen below):</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/allow-pasting-colab-console.jpg" alt="allow-pasting-colab-console"></p>
<p>Now let's start training! Simply find the <strong>Runtime</strong> tab near the top left corner of the page and select <strong>Run All</strong>. Follow through each code snippet until you reach step 1.2 where you will have to authorize the notebook to access your Google Drive. Just click the link it provides you and copy &amp; paste the authorization code in the input field.</p>
<p>You can then let it the notebook run until it finishes training.</p>
<p>Take a look at the Predict and Evaluate section too see the results:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/results1.jpg" alt="results1"></p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/results2.jpg" alt="results2"></p>
<p>Now that we are done training, we can enjoy another meme:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/meme2.jpg" alt="meme2"></p>
<p>###Step 3, Setting Up The Jetson Nano:</p>
<p>First, follow NVIDIA's tutorial, <a href="https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#intro">Getting Started With Jetson Nano</a>.</p>
<p>After booting up, open a new terminal and download the necessary packages and python modules (<strong>Note:</strong> this will take a LONG time!):</p>
<pre><code class="hljs css language-console"><span class="hljs-meta">$</span><span class="bash"> sudo apt-get update</span>
<span class="hljs-meta">$</span><span class="bash"> sudo apt-get upgrade</span>
<span class="hljs-meta">$</span><span class="bash"> sudo apt-get install python3-pip gcc-8 g++-8 libopencv-dev</span>
<span class="hljs-meta">$</span><span class="bash"> sudo apt-get install libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev gfortran libopenblas-dev liblapack-dev</span>
<span class="hljs-meta">$</span><span class="bash"> sudo pip3 install -U pip testresources setuptools cython</span>
<span class="hljs-meta">$</span><span class="bash"> sudo pip3 install -U numpy==1.16.1 future==0.17.1 mock==3.0.5 h5py==2.9.0 keras_preprocessing==1.0.5 keras_applications==1.0.8 gast==0.2.2 enum34 futures protobuf</span>
<span class="hljs-meta">$</span><span class="bash"> sudo pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v43 tensorflow-gpu==2.0.0+nv20.1</span>
</code></pre>
<p>This will install OpenCV 4.0 for computer vision, gcc-8 &amp; g++-8 for C++ compiling, and TensorFlow 2.0 to run our handwritten text recognition model. Those are the main ones, but we will also need to install all the other required dependencies:</p>
<pre><code class="hljs css language-console"><span class="hljs-meta">$</span><span class="bash"> sudo pip3 uninstall enum34</span>
<span class="hljs-meta">$</span><span class="bash"> sudo apt-get install python3-matplotlib python3-numpy python3-pil python3-scipy nano</span>
<span class="hljs-meta">$</span><span class="bash"> sudo apt-get install build-essential cython</span>
<span class="hljs-meta">$</span><span class="bash"> sudo apt install --reinstall python*-decorator</span>
<span class="hljs-meta">$</span><span class="bash"> sudo pip3 install -U scikit-image</span>
<span class="hljs-meta">$</span><span class="bash"> sudo pip3 install -U google-cloud-vision google-cloud-texttospeech imutils pytesseract pyttsx3 natsort playsound </span>
<span class="hljs-meta">$</span><span class="bash"> sudo pip3 install -U autopep8==1.4.4 editdistance==0.5.3 flake8==3.7.9 kaldiio==2.15.1</span>
</code></pre>
<p>We also need to install llvmlite from source in order to install numba (<strong>Note:</strong> This will take an even LONGER time. Treat your self to a nice movie!):</p>
<pre><code class="hljs css language-console"><span class="hljs-meta">$</span><span class="bash"> wget http://releases.llvm.org/7.0.1/llvm-7.0.1.src.tar.xz</span>
<span class="hljs-meta">$</span><span class="bash"> tar -xvf llvm-7.0.1.src.tar.xz</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> llvm-7.0.1.src</span>
<span class="hljs-meta">$</span><span class="bash"> mkdir llvm_build_dir</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> llvm_build_dir/</span>
<span class="hljs-meta">$</span><span class="bash"> cmake ../ -DCMAKE_BUILD_TYPE=Release -DLLVM_TARGETS_TO_BUILD=<span class="hljs-string">"ARM;X86;AArch64"</span></span>
<span class="hljs-meta">$</span><span class="bash"> make -j4</span>
<span class="hljs-meta">$</span><span class="bash"> sudo make install</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> bin/</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">echo</span> <span class="hljs-string">"export LLVM_CONFIG=\""</span>`<span class="hljs-built_in">pwd</span>`<span class="hljs-string">"/llvm-config\""</span> &gt;&gt; ~/.bashrc</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">echo</span> <span class="hljs-string">"alias llvm='"</span>`<span class="hljs-built_in">pwd</span>`<span class="hljs-string">"/llvm-lit'"</span> &gt;&gt; ~/.bashrc</span>
<span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">source</span> ~/.bashrc</span>
<span class="hljs-meta">$</span><span class="bash"> sudo pip3 install -U llvmlite numba</span>
</code></pre>
<p>Next, you need to plug in the WiFi USB adapter into any of the USB ports:</p>
<p><img src="/docs/assets/tutorials/reading-eye-for-the-blind/wifi-usb.jpg" alt="wifi-usb"></p>
</span></div></article></div><div class="docs-prevnext"><a class="docs-next button" href="/docs/tutorials/ft-explorer-vtol-tutorial"><span>FT Explorer VTOL Build</span><span class="arrow-next"> →</span></a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#story">Story</a><ul class="toc-headings"><li><a href="#demo-video">Demo Video:</a></li><li><a href="#code">Code:</a></li><li><a href="#step-1-setting-up-your-computer">Step 1, Setting Up Your Computer:</a></li><li><a href="#step-2-training-the-handwritten-text-recognition-model">Step 2, Training the Handwritten Text Recognition Model:</a></li></ul></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="/" class="nav-home"><img src="/img/favicon.ico" alt="Andrew Bernas" width="66" height="58"/></a><div><h5>Projects</h5><a href="/docs/doc1.html">In Progress</a><a href="/docs/engineering-projects/recycle-sorting-robot.html">Engineering Projects</a><a href="/docs/doc3.html">Conservation Service Projects</a></div><div><h5>Tutorials</h5><a href="/doc3.html">Work in progress!!!</a><a href="https://stackoverflow.com/questions/tagged/" target="_blank" rel="noreferrer noopener">Stack Overflow</a><a href="https://discordapp.com/">Project Chat</a><a href="https://twitter.com/" target="_blank" rel="noreferrer noopener">Twitter</a></div><div><h5>More</h5><a href="/blog">Blog</a><a href="https://github.com/bandofpv" target="_blank" rel="noreferrer noopener">GitHub</a><a class="github-button" data-icon="octicon-star" data-count-href="/bandofpv" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star</a></div></section><section class="copyright">Copyright © 2020 Andrew Bernas</section></footer></div></body></html>